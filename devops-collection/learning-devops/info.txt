The DevOps process is divided into several phases:
  A. Planning and prioritizing functionalities
  B. Development
  C. Continuous integration and delivery
  D. Continuous deployment
  E. Continuous monitoring

CI is an automatic process that allows you to check the completeness of an application's code every 
  time a team member makes a change. This verification must be done as quickly as possible.

Unlike CI, CD aims to test the entire application with all of its dependencies. This is very
  visible in microservice applications composed of several services and APIs; CI will only
  test the microservice under development, while once deployed in a staging environment,
  it will be possible to test and validate the entire application, as well as the APIs and
  microservices that it is composed of.

It is important that the package that's generated during CI, which will also be deployed
  during CD, is the same one that will be installed on all environments, and this should be
  the case until production. However, there may be configuration file transformations that
  differ, depending on the environment, but the application code (binaries, DLL, Docker
  images, and JAR) must remain unchanged.

-- Once continuous integration has been completed, the next step is to deploy the application automatically in 
   one or more non-production environments, which is called staging. This process is called continuous delivery (CD).

-- CD often starts with an application package being prepared by CI, which will be installed 
   based on a list of automated tasks. These tasks can be of any type: unzip, stop and restart 
   service, copy files, replace configuration, and so on. The execution of functional and
   acceptance tests can also be performed during the CD process.

The tools that are set up for CI/CD are often used with other solutions, as follows:
• A package manager: This constitutes the storage space of the packages generated by CI and recovered by CD. 
    These managers must support feeds, versioning, and different types of packages. There are several 
    on the market, such as Nexus, ProGet, Artifactory, and Azure Artifacts.
• A configuration manager: This allows you to manage configuration changes during
    CD; most CD tools include a configuration mechanism with a system of variables.

In CD, deploying the application in each staging environment is triggered as follows:
• It can be triggered automatically, following a successful execution in a previous
  environment. For example, we can imagine a case where the deployment in the pre-production environment is 
  automatically triggered when the integration tests have been successfully performed in a dedicated environment.
• It can be triggered manually, for sensitive environments such as the production environment, following manual 
  approval by the person responsible for validating the proper functionality of the application in an environment.

Continuous deployment
  Continuous deployment is an extension of CD, but this time, with a process that automates the entire CI/CD pipeline 
  from the moment the developer commits their code to deployment in production through all of the verification steps.

Continuous deployment can be implemented by using and implementing feature toggle techniques (or feature flags), 
  which involves encapsulating the application's functionalities in features and activating its features on 
  demand, directly in production, without having to redeploy the code of the application.
Another technique is to use a blue-green production infrastructure, which consists
  of two production environments, one blue and one green. First, we deploy to the blue
  environment, then to the green one; this will ensure that no downtime is required.



The benefits of IaC are as follows:
  • The standardization of infrastructure configuration reduces the risk of errors.
  • The code that describes the infrastructure is versioned and controlled in a source code manager.
  • The code is integrated into CI/CD pipelines.
  • Deployments that make infrastructure changes are faster and more efficient.
  • There's better management, control, and a reduction in infrastructure costs.


-- Provisioning is the act of instantiating the resources that make up the infrastructure. They
    can be of the Platform-as-a-Service (PaaS) and serverless resource types, such as a web
    app, Azure function, or Event Hub, but also the entire network part that is managed, such
    as VNet, subnets, routing tables, or Azure Firewall. For virtual machine resources, the
    provisioning step only creates or updates the VM cloud resource, but not its content.


Server configuration
  This step concerns configuring virtual machines, such as the hardening, directories, disk
  mounting, network configuration (firewall, proxy, and so on), and middleware installation
  There are different configuration tools, such as Ansible, PowerShell DSC, Chef, Puppet, and SaltStack


Protecting the state file with a remote backend
  When Terraform handles resources, it writes the state of these resources in a Terraform state file. This 
  file is in JSON format and preserves the resources and their properties throughout the execution of Terraform.
  By default, this file, called terraform.tfstate, is created locally when the first
  execution of the apply command is executed. Then, it will be used by Terraform each time
  the plan command is executed in order to compare its state (written in the state file) with
  that of the target infrastructure. Finally, it will return a preview of what will be applied

  When using Terraform in an enterprise, this locally stored state file poses many problems:
  • Knowing that this file contains the status of the infrastructure, it should not be
    deleted. If deleted, Terraform might not behave as expected when it is executed.
  • It must be accessible at the same time by all members of the team who are handling resources on the same infrastructure.
  • This file can contain sensitive data, so it must be secure.
  • When provisioning multiple environments, it is necessary to be able to use multiple state files.

  With all of these points, it is not possible to keep this state file locally or 
  even to archive it in an SCM. To solve this problem, Terraform allows this state 
  file to be stored in a shared and secure storage called the remote backend.

  In our case, we will use an azurerm remote backend to store our state files with a storage account and a blob 
  for the state file. Therefore, we will implement and use a remote backend in three steps:
 1. The creation of the storage account
 2. The Terraform configuration for the remote backend
 3. The execution of Terraform with the use of this remote backend

if multiple Terraform states are used to manage multiple environments,
it's possible to create several remote backend configurations with the simplified code in the .tf file:
╒══════════════════════╕
  terraform {          
     backend "azurerm" {} 
  }                    
╘══════════════════════╛
 

Terraform - Why a state file is required?
  Terraform has a very general design so that it can support as many different remote systems as possible. While it's 
  true that lots of remote objects have an obvious single ID which uniquely identifies them, there are some remote 
  object types which are uniquely identified by a combination of values and even some cases where the entire 
  configuration acts as the identity of the object.
  Also some remote systems have write-only attributes which the provider would have no way to recover if they were 
  not saved in the Terraform state. The state file includes a verbatim copy of the entire object that the 
  provider returned after the most recent action, and then when planning the next action Terraform sends 
  that entire object back to the provider. The provider itself then decides what subset of that data to use 
  to determine which remote object this resource instance represents.
  For many resource types you'll find that if you directly edit the state snapshots to remove everything except the 
  id attribute then the provider will still successfully refresh the object and repopulate all of the other data 
  you removed, but that isn't always true. Terraform itself doesn't know what a provider will need to do its work,
  and so it always just stores the entire object.
  Although that core problem of Terraform not knowing what is and is not significant is the original driver, the 
  fact that Terraform stores the entire object has some other benefits: 
  A provider can, in most cases, detect if an object has been changed outside of Terraform and add an additional 
  note to the plan if that might help explain why the provider is proposing to undo that change. Having this third 
  source of data to compare against gives Terraform a hint about whether a mismatch between the remote system and 
  the configuration was caused by changing the configuration or by changing the remote system.
  Extra features like terraform console and terraform show can allow inspecting the object data without having 
  to re-fetch everything from the source system.
  Terraform could in principle go and refresh all of the data again from the remote API, but since Terraform is 
  already storing a copy of the data anyway it's faster (and therefore more convenient when debugging) 
  to return that copy of the data.





Ansible features:
•• It is declarative and uses the easy-to-read YAML language.
•• Ansible only works with one executable.
•• It does not require agents to be installed on the VMs to be configured.
•• A simple SSL/WinRM connection is required for Ansible to connect to remote VMs.
•• It has a template engine and a vault to encrypt/decrypt sensitive data.
•• It is idempotent.

The main uses cases of Ansible are as follows:
•• Configuring a VM with middleware and hardening,
•• Infrastructure provisioning, such as Terraform, but using YAML configuration
•• Security compliance to test that the system or network configuration conforms to the enterprise requirements


To configure a system, Ansible needs several main artifacts:
• The hosts: These are target systems that Ansible will configure; the host can also be a local system.
• The inventory: This is a file in INI or YAML format that contains the list of target hosts that Ansible will 
  perform configuration actions on. This inventory can also be a script, which is the case with a dynamic inventory.
• The playbook: This is the Ansible configuration script that will be executed to configure hosts.


There are two types of inventories:
• Static inventory: Hosts are listed in a text file in INI (or YAML) format; this is the basic mode of Ansible 
  inventory. The static inventory is used in cases where we know the host addresses (IP or FQDN).
• Dynamic inventory: The list of hosts is dynamically generated by an external
  script (for example, with a Python script). The dynamic inventory is used if we do not have the addresses 
  of the hosts, for example, as with an infrastructure that is composed of on-demand environments.



Among the Infrastructure as Code (IaC) tools, there is Packer from the HashiCorp tools,
  which allows us to create VM images from a file (or template). It's an open source 
  command-line tool that allows us to create custom VM images of any OS 
  (these images are also called templates) on several platforms from a JSON file.
  
 ╒════════════════════════════════════════════════════════════════════════════════════════════════════╕
  ╒════════════════════════════════════════════════════════════════════════════════════════════════╕ 
    With these VM images created by Packer, we will be able to improve infrastructure provisioning     
    times with faster deployment, ready-to-use VMs, and, therefore, a reduction in downtime.           
  ╘════════════════════════════════════════════════════════════════════════════════════════════════╛ 
 ╘════════════════════════════════════════════════════════════════════════════════════════════════════╛
 
In Packer The provisioners section, which is optional, contains a list of scripts that will be executed 
by Packer on a temporary VM base image in order to build our custom VM image according to our needs.

As a reminder, to generate this image, Packer will, from our JSON template, create a
  temporary VM, on which it will perform all of the configuration actions described in this
  template, and then it will generate an image from this image. Finally, at the end of its
  execution, it removes the temporary VM and all of its dependencies.


in packer variables, eg:
╒════════════════════════════════════════╕
   "variables": {                         
      "clientid": "{{env `ARM_CLIENT_ID`}}", 
       ....                                   
    },
    "builders": {
      "client_id": "{{user `clientid`}}",
     }
╘════════════════════════════════════════╛
the clientid will take its value from the ARM_CLIENT_ID environment variable


GIT flow pattern 

1- First of all, we have the master branch, which contains the code that is currently in
   production. No developer is working directly on it.
2- From this master branch, we create a develop branch, which is the branch that
   will contain the changes to be deployed in the next delivery of the application.
3- For each of the application's functionalities, a feature/<name of the feature> 
   branch is created (/ will hence create a feature directory) from the develop branch.
4- As soon as a feature is finished being coded, the branch of the feature is merged into the develop branch.
5- Then, as soon as we want to deploy all of the latest features developed, we create a release branch from develop.
6- The release branch content is deployed in all environments successively.
7- As soon as the deployment in production has taken place, the release branch is merged with the master branch, 
   and, with this merge operation, the master branch contains the production code. Hence, the code that is on 
   the release branch and feature branch is no longer necessary, and these branches can be deleted.
8- If a bug is detected in production, we create a hotfix/<bug name> branch;
   then, once the bug is fixed, we merge this branch into the master and develop
   branches to propagate the fix on the next branches and deployments.

