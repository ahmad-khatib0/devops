Podman is an open-source alternative to Docker (its full name is the POD MANager). Its main advantage 
  is that you don’t need a service to be running like Docker on your machine; Podman is “daemonless.” 
  The command syntax is very similar to Docker’s. If you are concerned about security in your applications 
  or planning to use Kubernetes, Podman is a better alternative


The template code of the container is called a Dockerfile, which allows you to build a binary 
  version of the container, called an image. A more technology-agnostic alternative to Dockerfile 
  is a Containerfile, by the Open Container Initiative (OCI), part of The Linux Foundation. Both 
  are configuration files that automate the steps of creating a container image. The OCI promotes 
  open industry standards around container formats and runtimes. The OCI releases three specifications: 
  the Runtime Specification (runtime-spec), the Image Specification (image-spec), and the Distribution 
  Specification (distribution-spec). Dockerfiles and Containerfiles use a combination of system tools,
  system libraries, code, runtimes, and settings to specify how an application runs. A single
  image can be run as many times as you want in a local Docker or Kubernetes cluster.

Modern business applications require one or more of the following features:
  • High availability
  • Multi-cloud compatibility
  • Multi-tier storage
  • Elastic/auto-scaling
  • Self-healing
  • Security by design (DevSecOps) 

Container Security: 
  In the market, there are many open-source and commercially available products for this task nowadays.
  Popular tools include Anchore, Datadog Cloud SIEM, Sophos Cloud Native Security, Bitdefender 
  GravityZone, Sysdig Secure, Red Hat Advanced Cluster Security for Kubernetes, and Aqua Security. 
  Integrating one or more of these products into your DevSecOps process is a good way to address 
  and mitigate security risks.

Deploying in Operator
  Operator enables you to verify and deploy new applications in your Kubernetes cluster automatically. 
  You need some coding skills, because the code is going to rely on the Kubernetes Operator SDK, which 
  is a Minikube environment for running Kubernetes locally, as well as the Go language (Golang).
  
The seven steps needed to generate a Kubernetes Operator are as follows:
  1. Generate the boilerplate code using the Kubernetes Operator SDK and Minikube:
      $ minikube start init
      $ operator-sdk init
  2. Create the APIs and a custom resource:
  $  operator-sdk create api --version=v1alpha1 --kind=Traveller
  3. Download any dependencies.
  4. Create a deployment.
  5. Create a service.
  6. Add a reference in the controller for the deployment and service.
  7. Deploy the service, install the CRD, and deploy a CRD instance.
Operators extend Kubernetes APIs and create custom objects to customize the 
 cluster according to your needs.


Kubernetes Upgrade
  The plan begins with upgrading the control plane nodes. You need to set the cordon
  and drain node offline settings one by one, thus preventing scheduling and evicting any
  workloads, upgrade the kubelet and kubectl packages, restart the kubelet service, and
  bring the uncordon node back online. After upgrading the control plane nodes, you can upgrade 
  worker nodes. Begin by upgrading the kubeadm tool, executing the kubeadm upgrade node command, 
  setting the node in drain mode, upgrading kubelet and kubectl, and restarting the kubectl
  process, then you uncordon the node.

Configure an Ansible Execution Environment
  The Ansible Execution Environment is a way to containerize the execution of the Ansible
  invented by Red Hat. It maintains archives of the separation of the operating system
  dependencies, Python dependencies, and Ansible collections without interfering with
  your Linux system. It’s the evolution of Python Virtual Environment and can be executed
  natively in a Kubernetes cluster. It supersedes manual Python Virtual Environments,
  Ansible module dependencies, and Ansible Tower bubblewrap.
  Ansible Execution Environment relies on the Ansible Builder, the ansible-builder
  command, to create an Ansible Execution Environment. Ansible Builder produces a
  directory with the build context for the container image. It contains the Containerfile,
  along with any other files that need to be added to the image.
  The Ansible Runner, the ansible-runner command utility, executes the Ansible
  Playbook in an Ansible Execution Environment. The Ansible Runner enables you to run
  the Ansible Execution Environment as a container in the current machine.


The following stages apply to the SDLC methodology:
  1. Requirements analysis: Identify the problems and use cases.
  2. Planning: Define the costs and resources needed.
  3. Architecture design: Determine the design specification requirements.
  4. Software development: Build the software.
  5. Testing: Test for defects and deficiencies and verify the meeting of the specifications.
  6. Deployment: Launch the product to the customer and get feedback.


The twelve-factor application
  1. Codebase: Create one codebase tracked in revision control, many deploys.
  2. Dependencies: Explicitly declare and isolate dependencies.
  3. Config: Store the config in the environment.
  4. Backing services: Treat backing services as attached resources.
  5. Build, release, run: Strictly separate build and run stages.
  6. Processes: Execute the app as one or more stateless processes.
  7. Port binding: Export services via port binding.
  8. Concurrency: Scale out via the process model.
  9. Disposability: Maximize robustness with fast startup and graceful shutdown processes.
  10. Dev/prod parity: Keep development, staging, and production as similar as possible.
  11. Logs: Treat logs as event streams.
  12. Admin processes: Run admin/management tasks as one-off processes.
  

You can configure RBAC to ensure that only authorized users can access the resources in the 
  namespace. Once this is done, the Pod Security Standards will be applied at the Namespace Level. 
  Since Kubernetes version 1.23, Pod Security Admission (PSA) has been enabled in Kubernetes per cluster 
  and namespace lever by default. It enables you to use these built-in Pod Security Standards modes:
    • enforce (baseline)
    • audit (restricted)
    • warn (restricted)
  Using Kubernetes labels of the built-in Pod Security Admission, it is possible to enable pod security 
  standards at the namespace level. You can use the following label to set the pod security standard policy:
      pod-security.kubernetes.io/<MODE>: <LEVEL>
    • MODE sets the pod security standard modes: enforce, audit, warn
    • LEVEL sets the pod security standard levels: privileged, baseline, or restricted

Kubernetes has supported AppArmor since version 1.4. At the moment of writing this book, Kubernetes’ 
  annotation is the way to use the feature. When promoted to General Availability (GA), each 
  annotation will be a Kubernetes Object field. AppArmor is used to restrict a container’s access 
  to resources; you need to create an AppArmor profile for the container. This profile will define 
  the security rules for the containers, such as which files and directories it can access, which 
  system calls it can make, and which other containers or processes it can interact with. Additionally, 
  you need to configure the container runtime to ensure that the AppArmor profile is applied when the 
  container is started. Kubernetes-supported container runtimes that support AppArmor technology
  are Docker, CRI-O, and containerd. Once this is done, the container will be restricted to the access 
  rules defined in the AppArmor profile. 
# AppArmor profiles are specified per pod by adding the following annotation:
$ container.apparmor.security.beta.kubernetes.io/<container_name>:<profile_ref>
  You can deploy an application protected with AppArmor using the kubectl or Ansible 
  kubernetes.core.k8s module. There is no native Kubernetes way to load AppArmor profiles onto nodes. 
  However, you can use Ansible as an initialization script to enable it.

Security Pod Syscalls (seccomp)
  A system call, or syscall, is a direct request from your application to the operating system
  where it is executed. The typical use case is accessing hardware resources, launching threads, 
  communicating with other processes, and interacting with internal kernel services. You can use 
  the Linux secure computing mode, called seccomp, to enable a "secure" state one-way transition, 
  where it is possible to use only exit, sigreturn, read, and write on an already-open file 
  descriptor system call. Of course, you can restrict the syscalls of the container creating a 
  seccomp profile for the container. A profile defines a sandbox of the privileges of a process, 
  restricting the call from the userspace to the kernel. This profile will define the syscalls that the 
  container can make, as well as the parameters and arguments that can be used when making the syscalls.

Ansible Dynamic Inventory
  Ansible Dynamic Inventory enables you to generate Ansible Inventories automatically. The information 
  is read from external sources such as cloud providers, CMDBs, and inventory management systems. 
  This allows users to deploy infrastructure and applications dynamically and automate them quickly.
  The inventory is stored in a YAML file, which is then used by Ansible to determine which hosts to 
  target for tasks. Ansible Dynamic Inventory can be used for several different use cases, such 
  as dynamic scaling, provisioning, and application deployment. Ansible can also use multiple 
  inventory sources at the same time. You can mix and match dynamic and statically managed 
  inventory sources in the same Ansible run.


╭───────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                               │
│ The default behavior for Ansible to connect to any target hosts is to execute the             │
│ authentication phase and then copy a file generated by every single task of your              │
│ Ansible Playbook. Ansible connects to remote servers and executes code with the same          │
│ username. You can specify the connection username with the ansible_user variable              │
│ on the inventory. You can change the running user by specifying remote_user in the            │
│ playbook or globally in the ansible.cfg file. After a successful connection, Ansible          │
│ tries to create a temporary directory in the home of the user of the connection, where        │
│ it copies the task files if they doesn’t already exist. If Ansible is unable to create the    │
│ temporary directory, if that user does not have a home directory, or if their home            │
│ directory permissions do not allow them to write access, you can customize the path of        │
│ the temporary directory via the ansible.cfg file. For example, you can use the following      │
│ path in the /tmp directory:      remote_tmp = /tmp/.ansible-${USER}/tmp                       │
│ Ansible pipelining executes the Ansible modules on the target directly without the prior file │
│ transfer, consequently reducing the network operations. Another pleasant side-effect is the   │
│ increase in performance when enabled. By default, Ansible pipelining is disabled. You can     │
│ enable it using the ANSIBLE_PIPELINING=True environment variable or setting the               │
│ pipelining=true key in the [connection] and [defaults] sections of the ansible.cfg file.      │
│                                                                                               │
╰───────────────────────────────────────────────────────────────────────────────────────────────╯

Taint Nodes
  Taints and tolerations are a flexible way to steer pods away from nodes or evict pods that shouldn’t
  be running. A typical use case of the taint node is to take advantage of particular nodes with 
  special hardware (such as GPUs) to run only pods that require the use of the specialized hardware 
  and keep out the pods that don’t require it. Moreover, you can use this tool when you need to 
  execute maintenance in your Kubernetes cluster. Kubernetes Taint is used to mark a node as unable 
  to schedule any pods that do not tolerate the taint. This is useful when you need to perform 
  maintenance on a node, as it ensures that any existing pods running on the node will remain running 
  and any new pods will be rescheduled onto other nodes in the cluster. The taint command works
  by adding a label to the node, which is then used to identify which pods should not be
  scheduled onto the node. This label can be removed when the maintenance is complete,
  allowing the node to start accepting new pods again.

Drain, Cordon, or Uncordon Nodes
  Ansible for K8s Management Kubernetes Drain is a tool used to safely evict all of the pods from 
  a node before performing maintenance on the node. Note that it’s always a good practice to have a
  planned maintenance window. The Drain operation is often combined with the Taint operation.
  When you run the drain command, Kubernetes will attempt to delete all of the pods on the node 
  except for mirror pods, which are created by replication controllers, jobs, or daemon sets. The 
  drain command also allows you to specify a PodDisruptionBudget, which will ensure that the number 
  of pods running on the node does not drop below the specified minimum during the eviction process. 
  Once the eviction process is complete, you can then safely perform maintenance on the node, knowing 
  that the pods will be safely relocated to other nodes in the cluster. Kubernetes Cordon marks a 
  node as “unschedulable,” meaning that no new pods will be scheduled onto the node. This is useful 
  when you need to perform maintenance on a node, as it ensures that any existing pods running on 
  the node will remain running and any new pods will be rescheduled onto other nodes in the cluster. 
  The cordon command works by setting the node’s unschedulable flag to true, which will prevent the
  Kubernetes scheduler from placing any new pods on the node. Once the maintenance is complete, 
  the unschedulable flag can be reset back to false, allowing the node to start accepting new pods 
  again. Kubernetes Uncordon is used to mark a node as schedulable, meaning that new pods can be 
  scheduled onto the node. This is the opposite of the kubectl cordon command, which marks a node as 
  unschedulable, thus preventing new pods from being scheduled onto that node. The uncordon command 
  works by setting the node’s unschedulable flag to false, allowing the Kubernetes scheduler to 
  place new pods on the node. This is useful when maintenance has been completed on a node, and 
  you want to allow new pods to start running on the node again.


K8s Security
The reference for Kubernetes in this field are the four Cs of cloud-native security:
• Cloud
• Cluster
• Container
• Code

To begin with, the cloud infrastructure that hosts your cluster needs to be adequately secured 
  and enable access to ports on the cluster only by trusted networks. By default, Container Engines 
  are open from anywhere. A network firewall could be helpful in mitigating this behavior. This 
  prevents a potential attacker from running port scans to detect potential open ports in your 
  systems and connect to them. This is the first “C” in cloud-native and it impacts the security 
  of the entire infrastructure, even when the cluster is in a private or public cloud data center.
  The next is cluster security, which involves the ability of a potential attacker to execute 
  commands in your cluster. A potential attacker could have the ability to access running Container 
  Engines such as Docker Daemons, exposed publicly, as well as the Kubernetes Dashboard without the 
  proper authentication or authorization mechanism. This can be prevented by using network policies 
  and security in ingress. The next “C” is containers. A potential attacker can run any container of 
  choice without restriction, even in privileged mode, no matter what repository it came from or what 
  tag is used. Moreover, the attacker can install any application when it can run a malicious container 
  in your cluster without restriction. A few years ago, the Dirty Cow CVE-2016-5195 exploited a bug 
  in the Linux kernel’s memory subsystem copy-on-write (COW) to gain write access from the container 
  to the host system, increasing the privileges. You can prevent this behavior by enabling only running 
  containers from a secure internal registry, disallowing running in privileged mode, and sandboxing 
  isolating each other. The best way to prevent supply chain attacks and minimize microservice 
  vulnerability is acting in the code. Avoid hard-coding database credentials, thereby passing critical 
  pieces of information through environment variables or exposing applications without TLS/SSL. These 
  are bad coding practices. The DevSecOps movement applies security as soon as possible in the code 
  design phase of your development.
  Another important tool is to scan for vulnerabilities as soon you create your container image, 
  when you push to the container image. You must apply this methodology as soon as possible during 
  the development process. You can also reduce the attack surface of container images using the 
  smallest images possible, removing all unnecessary binaries. Reducing the attack surface should be
  your mantra. A good security practice is to build your images using scratch. Add a USER directive to
  run as a non-root user inside a container. You can sign your images in order to run only trusted 
  containers. Most of the time, you derive your containers using the FROM directive. The most secure 
  way is to combine the scratch with a multi-stage build. In this way, you obtain a shirked image with 
  only your application
  

AAA
  Every time you make a request, Kubernetes determines if it is allowed or denied using Authentication, 
  Authorization, and Accounting (AAA). This is how your Kubernetes cluster verifies who you are and 
  what you are authorized to execute. Policies define permissions according to role-based access 
  control (RBAC) or attribute-based access control (ABAC). Associated policies define Kubernetes 
  resources. Kubernetes RBAC is the most commonly used method to perform AAA tasks. Policies are 
  in place to make decisions to allow or deny an action. The following acronyms are used in the 
  AAA context:
      • authn: Authentication (who are you?)
      • authz: Authorization (what are you authorized to do?)
      • auth: Accounting (what area are you authorized to access?)
  You can use Kubernetes RBAC to provide authorization to resources in your Kubernetes cluster.
  
Kubernetes hosts a per-cluster, public OpenID Connect (OIDC) endpoint that
  contains signing keys for JSON web tokens allowing external systems, like Amazon IAM, to validate 
  and accept the Kubernetes-issued OIDC tokens. OIDC federation access allows you to assume RBAC 
  roles via the Secure Token Service (STS), enabling authentication with an OIDC provider and 
  receiving a JSON Web Token (JWT), which is used to assume a role. OpenID Connect allows single 
  sign-on (SSO) for logging into your Kubernetes cluster. You can use an existing public OpenID 
  Connect Identity Provider (such as Google, GitHub, Microsoft, Amazon, and so on), or you can run 
  your own identity providers, such as dex, Keycloak, CloudFoundry UAA, or Tremolo Security’s OpenUnison.

Calico
  You can secure your network communications using Calico around your pod. Calico
  acts like a firewall in your pod. It enables you to use network policies within a pod and
  apply the zero-trust security model. This means that it considers all the traffic dangerous
  by default, and you need to manually set an allow list. The easiest way to install Calico
  is via the Tigera operator. The Tigera operator provides lifecycle management for Calico
  exposed via the Kubernetes API, defined as a custom resource definition.




