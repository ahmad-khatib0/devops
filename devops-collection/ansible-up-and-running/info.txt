Canary release is a technique to reduce the risk of introducing a new software version in 
  production by slowly rolling out the change to a small subset of users before rolling 
  it out to the entire infrastructure and making it available to everybody. 

Reorganize the Ansible content into three parts:
  • Core components, created by the Ansible team
  • Certified content, created by Red Hat’s business partners
  • Community content, created by thousands of enthusiasts worldwide


Where Should I Put My ansible.cfg File?
Ansible looks for an ansible.cfg file in the following places, in this order:
  • File specified by the ANSIBLE_CONFIG environment variable
  • ./ansible.cfg (ansible.cfg in the current directory)
  • ~/.ansible.cfg (.ansible.cfg in your home directory)
  • /etc/ansible/ansible.cfg (Linux) or /usr/local/etc/ansible/ansi‐ble.cfg (*BSD)


Ansible uses /etc/ansible/hosts as the default location for the inventory file. 
Keeping the inventory in the same directory as the play‐ books and so on gives you 
the possibility of a specific inventory per project instead of just a global one.

We use the .j2 extension to indicate that the file is a Jinja2 template. However, you can
use a different extension if you like; Ansible doesn’t care.

In ch3/playbooks/templates/nginx.conf.j2 file 
server_name
  The hostname of the web server (such as www.example.com)
cert_file
  The filename of the TLS certificate
key_file
  The filename of the TLS private key
tls_dir
  The directory with the above files



You don’t need to install Ansible on your machine to use it when you have config.vm.provision "ansible_local" 
in your Vagrantfile; it will be installed and run in the virtual machine. When you use 
config.vm.provision "ansible" in the Vagrantfile, the provisioner does use Ansible on your machine.

-- The vagrant-hostmanager plug-in helps in addressing multiple virtual machines by hostname.
-- The vagrant-vbguest plug-in works on VirtualBox and can automatically install or
   update Guest Additions in your guest virtual machines

-- An Ansible convention is to copy files from a subdirectory named files, and to source Jinja2 
   templates from a subdirectory named templates. Ansible searches these directories automatically.

-- End of File:  YAML files may end with three dots, which can be useful to 
   prove completeness. But quite often this practice is skipped.

-- Booleans:  YAML has a native Boolean type and provides you with a variety of values that
   evaluate to true or false. For example, these are all Boolean true values in YAML:
   true, True, TRUE, yes, Yes, YES, on, On, ON
   These are all Boolean false values in YAML:
   false, False, FALSE, no, No, NO, off, Off, OFF

--  YAML also supports an inline format for lists, with comma-separated values in square brackets:
    shows: [ My Fair Lady , Oklahoma , The Pirates of Penzance ]
    
-- YAML also supports an inline format for dictionaries, with comma-separated tuples in braces:
    address: { street: Main Street, appt: '742', city: Logan, state: Ohio}

-- Multiline Strings
    You can format multiline strings with YAML by combining a block style indicator
    (| or >), a block chomping indicator (+ or –), and even an indentation indicator
    (1 to 9). For example, when we need a preformatted block, we use the pipe character with a plus sign (|+),
    So The YAML parser will keep all line breaks as you enter them:
  ---
  visiting_address: |+
    Department of Computer Science
    A.V. Williams Building
    University of Maryland
  city: College Park
  state: Maryland

-- If you reference a variable right after specifying the module, the YAML parser will misinterpret the 
   variable reference as the beginning of an inline dictionary. Consider the following example:
- name: Perform some task
   command: {{ myapp }} -a foo
Ansible will try to parse the first part of {{ myapp }} -a foo as a dictionary instead
of a string, and will return an error. In this case, you must quote the arguments:
- name: Perform some task
    command: "{{ myapp }} -a foo"






become:
  If this Boolean variable is true, Ansible will become the become_user to run tasks. This is useful when 
  managing Linux servers, since by default you should not log in as the root user. become can be specified 
  per task, or per play, as needed, and become_user can be used to specify root (the default if omitted) or
  another user, yet become is subject to your system’s policies. 
  A sudoers file might need to be adjusted to be able to become root.

package
  Installs or removes packages by using the host’s package manager
copy
  Copies a file from the machine where you run Ansible to the web servers
file
  Sets the attribute of a file, symlink, or directory
service
  Starts, stops, or restarts a service
template
  Generates a file from a template and copies it to the hosts

When you want to run a task with each item from a list, you can use loop. A loop executes the task 
  multiple times, each time replacing item with different values from the specified list:
loop:
  - "{{ key_file }}"
  - "{{ cert_file }}"
notify: Restart nginx


Handlers are one of the conditional forms that Ansible supports. A handler is similar to a task, but it 
  runs only if it has been notified by a task. A task will fire the notification if Ansible recognizes 
  that the task has changed the state of the system. If a play contains multiple handlers, the handlers 
  always run in the order that they are defined in the handlers section, not the notification order. 
  They run only once, even if they are notified multiple times. The official Ansible documentation mentions 
  that the only common uses for han‐ dlers are reboots and restarting services. Lorin uses them only for 
  restarting serv‐ ices—he thinks it’s a pretty small optimization to restart only once on change, since
  we can always just unconditionally restart the service at the end of the playbook, and
  restarting a service doesn’t usually take very long. But when you restart NGINX, you
  might affect user sessions; notifying handlers help avoid unnecessary restarts. Bas
  likes to validate the configuration before restarting, especially if it’s a critical service
  like sshd. He has handlers notifying handlers.
  
Testing
One pitfall with handlers is that they can be troublesome when debugging a play‐
book. The problem usually unfolds something like this:
• You run a playbook.
• One of the tasks with a notify on it changes state.
• An error occurs on a subsequent task, stopping Ansible.
• You fix the error in your playbook.
• You run Ansible again.
• None of the tasks reports a state change the second time around, so Ansible doesn’t run the handler.



The Ansible inventory is a very flexible object: it can be a file (in several formats), a directory, or an 
  executable, and some executables are bundled as plug-ins. Inventory plug-ins allow us to point at data 
  sources, like your cloud provider, to compile the inventory. An inventory can be stored separately 
  from your playbooks. This means that you can create one inventory directory to use with Ansible on 
  the command line, with hosts running in Vagrant, Amazon EC2, Google Cloud Platform, or Microsoft
  Azure, or wherever you like!

Ansible automatically adds one host to the inventory by default: localhost. It understands that localhost 
refers to your local machine, with which it will interact directly rather than connecting by SSH.

ansible_connection
  Ansible supports multiple transports, which are mechanisms that Ansible uses
  to connect to the host. The default transport, smart, will check whether the
  locally installed SSH client supports a feature called ControlPersist. If the SSH
  client supports ControlPersist, Ansible will use the local SSH client. If not, the
  smart transport will fall back to using a Python-based SSH client library called Paramiko.
  
ansible_shell_type
  Ansible works by making SSH connections to remote machines and then invoking scripts. By default, 
  Ansible assumes that the remote shell is the Bourne shell located at /bin/sh, and will generate 
  the appropriate command-line parameters that work with that. It creates temporary directories to 
  store these scripts. Ansible also accepts csh, fish, and (on Windows) powershell as valid values for
  this parameter. Ansible doesn’t work with restricted shells.
  
ansible_python_interpreter
  Ansible needs to know the location of the Python interpreter on the remote
  machine. You might want to change this to choose a version that works for you.
  The easiest way to run Ansible under Python 3 is to install it with pip3 and set this:
  ansible_python_interpreter="/usr/bin/env python3"
  
ansible_*_interpreter
  If you are using a custom module that is not written in Python, you can use this
  parameter to specify the location of the interpreter (such as /usr/bin/ruby)

Behavioral inventory parameter        ansible.cfg option
ansible_port                          remote_port
ansible_user                          remote_user
ansible_ssh_private_key_file          ssh_private_key_file
ansible_shell_type                    executable (see the following paragraph)

The ansible.cfg executable config option is not exactly the same as the ansi
  ble_shell_type behavioral inventory parameter. The executable specifies the full path of the shell 
  to use on the remote machine (for example, /usr/local/bin/fish). Ansible will look at the base name 
  of this path (in this case fish) and use that as the default value for ansible_shell_type.



Ansible also supports using <hostname>:<port> syntax when specifying hosts, so we
could replace the line that contains vagrant1 with 127.0.0.1:2222
[vagrant]
127.0.0.1:2222
127.0.0.1:2200
127.0.0.1:2201

However, we can’t actually run what you see in above, The reason is that Ansible’s inventory can associate 
only a single host with 127.0.0.1, so the Vagrant group would contain only one host instead of three.

The “cattle” approach to servers is much more scalable, and Ansible supports it well by supporting numeric 
  patterns. For example, if your 20 servers are named web1.example.com, web2.example.com, and so on, 
  then you can specify them in the inventory file like this:
[web]
web[1:20].example.com

If you prefer to have a leading zero (such as web01.example.com), specify that in the range, like this:
[web]
web[01:20].example.com

Ansible also supports using alphabetic characters to specify ranges. If you want to use the convention 
  web-a.example.com, web-b.example.com, and so on, for your 20 servers, then you can do this:
[web]
web-[a:t].example.com

We could define a variable named color and set it to a value for each server:
  amsterdam.example.com color=red
  seoul.example.com color=green
  sydney.example.com color=blue
  toronto.example.com color=purple

For example, if Lorin has a directory containing his playbooks at /home/lorin/playbooks/ with 
  an inventory directory and hosts file at /home/lorin/inventory/hosts, he
  should put variables for the amsterdam.example.com host in the file /home/lorin/
  inventory/host_vars/amsterdam.example.com and variables for the production group
  in the file /home/lorin/inventory/group_vars/production
If we want to break things out even further, Ansible lets us define group_vars/produc‐
  tion as a directory instead of a file. We can place multiple YAML files into it that
  contain variable definitions. For example, we could put database-related variables in
  one file and the RabbitMQ-related variables in another file

If the inventory file is marked executable, Ansible will assume it is a dynamic 
  inventory script and will execute the file instead of reading it.

An Ansible dynamic inventory script must support two command-line flags:
  • --host=<hostname> for showing host details
  • --list for listing groups

Ansible’s group_by module allows you to create new groups while a playbook is
  executing. Any group you create will be based on the value of a variable that has been
  set on each host, which Ansible refers to as a fact

When Ansible gathers facts, it connects to the hosts and queries it for all kinds
  of details about the hosts: CPU architecture, operating system, IP addresses, mem‐
  ory info, disk info, and more. You can access this data in the ansible_facts vari‐
  able. By default, you can also access some Ansible facts as top-level variables with
  ansible_ prefix, and they behave just like any other variable. You can disable this
  behavior using the INJECT_FACTS_AS_VARS setting.


Because updating the cache takes additional time, and because we might be running
  a playbook multiple times in quick succession to debug it, we can avoid paying the
  cache update penalty by using the cache_valid_time argument to the module. This
  instructs to update the cache only if it’s older than a certain threshold. 

SSH agent forwarding allows you to use your private, local SSH key remotely without worrying about leaving 
confidential data on the server you're working with. It's built into ssh, and is easy to set up and use.

What’s nip.io?
  You might have noticed that the domains we are using look a little strange:
  192.168.33.10.nip.io and www.192.168.33.10.nip.io. They are domain names, but they
  have the IP address embedded within them.
  When you access a website, you pretty much always point your browser to a domain
  name, such as http://www.ansiblebook.com, instead of an IP address, such as http://
  151.101.192.133. When we write our playbook to deploy Mezzanine to Vagrant, we
  want to configure the application with the domain name or names by which it should be accessible.
  The problem is that we don’t have a DNS record that maps to the IP address of
  our Vagrant box. In this case, that’s 192.168.33.10. There’s nothing stopping us from
  setting up a DNS entry for this. For example, we could create a DNS entry from
  mezzanine-internal.ansiblebook.com that points to 192.168.33.10.
  However, if we want to create a DNS name that resolves to a particular IP address,
  there’s a convenient service called nip.io, provided free of charge by Exentrique
  Solutions, that we can use so that we don’t need to create our own DNS records.
  If AAA.BBB.CCC.DDD is an IP address, the DNS entry AAA.BBB.CCC.DDD.nip.io
  will resolve to AAA.BBB.CCC.DDD. For example, 192.168.33.10.nip.io resolves to
  192.168.33.10. In addition, www.192.168.33.10.nip.io also resolves to 192.168.33.10.
  I find nip.io to be a great tool when I’m deploying web applications to private IP
  addresses for testing purposes. Alternatively, you can simply add entries to the /etc/
  hosts file on your local machine, which also works when you’re offline.
  Let’s examine the Jinja2 for loop syntax. To make things a little easier to read, we’ll
  break it up across multiple lines, like this:


Django applications use a special script called manage.py that performs administrative 
  actions for Django applications such as the following:
• Creating database tables
• Applying database migrations
• Loading fixtures from files into the database
• Dumping fixtures from the database to files
• Copying static assets to the appropriate directory


We usually start by running molecule converge several times to get the Ansible role
  just right. Converge runs the converge.yml playbook that molecule init created. If
  there is a pre-condition for the role, like another role to run first, then it makes sense
  to create a prepare.yml playbook to save time during development. When using the
  delegated driver, create a cleanup.yml playbook. You can call these extra playbooks
  with molecule prepare and molecule cleanup, respectively.


Verifiers
  Verifiers are tools used to assert the success of running the role in a playbook. While
  we know that each module of Ansible has been tested, the outcome of a role is not
  guaranteed. It is good practice to automate tests that validate the outcome. There are
  three verifiers available for use with Molecule:
  Ansible
    The default verifier
  Goss
    A third-party verifier based on YAML specifications
  TestInfra
    A Python test framework
    
  The Goss and TestInfra verifiers use the files from the tests subdirectory of a molecule
  scenario, test_default.yaml for Goss and test_default.py for TestInfra.

Ansible
  You can use a playbook named verify.yml to verify the results of the converge and
  idempotence steps once they have finished. Just use Ansible modules like wait_for,
  package_facts, service_facts, uri, and assert to test the outcomes. To do so, use:
  $ molecule verify

TestInfra
  If you have advanced testing requirements, it’s helpful to have a Python-based test
  framework. With TestInfra, you can write unit tests in Python to verify the actual
  state of your Ansible-configured servers. TestInfra aspires to be the Python equivalent
  of the Ruby-based ServerSpec, which gained popularity as a test framework for
  systems managed with Puppet.


IaaS clouds typically use virtual machines to implement the servers, although you
  can build an IaaS cloud by using bare-metal servers (where users run directly on the
  hardware rather than inside a virtual machine) or containers. Most IaaS clouds let
  you do more than just start up and tear down servers. In particular, they typically
  let you provision storage so you can attach and detach disks to and from your
  servers. This type of storage is commonly referred to as block storage. They also offer
  networking features, so you can define network topologies that describe how your
  servers are interconnected, as well as firewall rules or security groups that restrict
  networking to and from your servers.
  
  The next layer in a cloud consists of specific innovations developed by cloud service
  providers and application runtimes like container clusters, application servers, ser‐
  verless environments, operating systems, and databases. This layer is called platform
  as a service (PaaS). You manage your applications and data; the platform manages
  the rest. PaaS allows distinctive features that are a point of competition among cloud
  providers, especially since competing over cost-efficiency in IaaS is a race to the
  bottom. However, the Kubernetes container platform, a common platform in any
  cloud, has seen the greatest interest.
  Any app that runs in the cloud has many layers, but when only one is visible to the
  cloud customer (or their customers) it is software as a service (SaaS). They just use
  the software, unaware of the servers’ whereabouts.


EC2 documentation interchangeably uses the terms creating instances, launching
  instances, and running instances to describe the process of bringing up a new instance.
  However, starting instances means something different—starting up an instance that
  had previously been put in the stopped state.

Each AMI has an associated identifier string, called an AMI ID, which starts with
  ami- and then has hexadecimal characters—for example, ami-1234567890abcdef0.
  Prior to January 2016, the IDs assigned to newly created AMIs used eight characters
  after the hyphen (for example, ami-1a2b3c4d). Between January 2016 and June 2018,
  Amazon was in the process of changing the IDs of all these resource types to use
  17 characters after the hyphen. Depending on when your account was created, you
  might have resources with short IDs, though any new resources of these types receive the longer IDs. 

All the bits of Ansible that interact with EC2 talk to the EC2 API. 
  The API does not use a username and password for credentials. Instead, it uses two
  strings: an access key ID and a secret access key.
  These strings typically look like this:
  • Sample EC2 access key ID: AKIAIOSFODNN7EXAMPLE
  • Sample EC2 secret access key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY


Virtual Private Clouds
  When Amazon first launched EC2 back in 2006, all of the EC2 instances were
  effectively connected to the same flat network.3 Every EC2 instance had a private IP
  address and a public IP address. In 2009, Amazon introduced a feature called Virtual
  Private Cloud (VPC). VPC allows users to control how their instances are networked
  together and whether they will be publicly accessible from the internet or isolated.
  Amazon uses the term VPC to describe the virtual networks that users can create
  inside EC2. Think of a VPC as an isolated network. When you create a VPC, you
  specify an IP address range. It must be a subset of one of the private address ranges
  (10.0.0.0/8, 172.16.0.0/12, or 192.168.0.0/16).

Permitted IP Addresses
  Security groups allow you to restrict which IP addresses may connect to an instance.
  You specify a subnet by using classless interdomain routing (CIDR) notation. An
  example of a subnet specified with CIDR notation is 203.0.113.0/24,6 which means
  that the first 24 bits of the IP address must match the first 24 bits of 203.0.113.0.
  People sometimes just say “/24” to refer to the size of a CIDR that ends in /24.
  A /24 is a nice value because it corresponds to the first three octets of the address,
  namely 203.0.113.7 What this means is that any IP address that starts with 203.0.113
  is in the subnet, that is, any IP address in the range 203.0.113.0 to 203.0.113.255. Be
  aware that addresses 0 and 255 are not allowed for hosts.
  If you specify 0.0.0.0/0, any IP address may connect.


CALLBACK PLUG-INS
Ansible supports a feature called callback plug-ins that can perform custom actions
  in response to Ansible events, such as a play starting or a task completing on a host.
  You can use a callback plug-in to do things such as send a Slack message or write an
  entry to a remote logging server. In fact, the output you see in your terminal when
  you execute an Ansible playbook is implemented as a callback plug-in.
  Ansible supports three kinds of callback plug-ins:
  • Stdout plug-ins
  • Notification plug-ins
  • Aggregate plug-ins
Stdout plug-ins control the format of the output displayed to the terminal. Ansible’s
  implementation makes no distinction between notification and aggregate plug-ins,
  which can perform a variety of actions.
  
ARA
  ARA Records Ansible (ARA, another recursive acronym) is more than just a callback
  plug-in. It provides reporting by saving detailed and granular results of ansible and
  ansible-playbook commands wherever you run them 

debug
  The debug plug-in makes it easier to read stdout (normal output of commands)
  and stderr (error output of commands) returned by tasks, which can be helpful for
  debugging. The default plug-in can make it difficult to read the output:

dense
  The dense plug-in (new in Ansible 2.3) always shows two lines of output. It over‐
  writes the existing lines rather than scrolling:

json
  The json plug-in generates machine-readable JSON as output. This is useful if you
  want to process the Ansible output by using a script. Note that this callback will not
  generate output until the entire playbook has finished executing.

null
  The null plug-in shows no output at all.

minimal
  The minimal plug-in does very little processing of the result Ansible returns from an event


You can think of modules as the “verbs” of the Ansible “language”—without them, the YAML would 
  not do anything. Ansible modules are programmed in Python for Linux/BSD/Unix machines and 
  in PowerShell for Windows machines, but in principle they can be written in any language. 
  
How Ansible Invokes Modules: 
  1. Generate a standalone Python script with the arguments (Python modules only)
  2. Copy the module to the host
  3. Create an arguments file on the host (non-Python modules only)
  4. Invoke the module on the host, passing the arguments file as an argument
  5. Parse the standard output of the module

We can tell Ansible to generate the arguments file for the module as JSON, 
  by adding the following line to playbooks/library/can_reach:  # WANT_JSON


Output Variables That Ansible Expects: 
changed
All Ansible modules should return a changed variable. The changed variable is a Boolean that 
  tells whether the module execution caused the host to change state. When Ansible runs, 
  it will show in the output whether a state change has happened. If a task has a notify 
  clause to notify a handler, the notification will fire only if changed is true.

failed
  If the module fails to complete, it should return "failed": true. Ansible will treat
  this task execution as a failure and will not run any further tasks against the host that
  failed unless the task has an ignore_errors or failed_when clause. If the module succeeds, 
  you can either return "failed": false or you can simply leave out the variable.

msg
  Use the msg variable to add a descriptive message that describes the reason that a
  module failed. If a task fails, and the module returns a msg variable, then Ansible will 
  output that variable slightly differently than it does the other variables. For example, 
  if a module returns the following:
    {"failed": true, "msg": "could not reach www.example.com:81"}
  then Ansible will output the following lines when executing this task:
    failed: [fedora] ==> {"failed": true}
    msg: could not reach www.example.com:81
  After a host fails, Ansible tries to continue with the remaining hosts that did not fail.

When Ansible runs a playbook in check mode, it will not make any changes to
  the hosts when it runs. Instead, it will simply report whether each task would have
  changed the host, returned successfully without making a change, or returned an error.


When Ansible runs a playbook it makes many SSH connections, to do things such as
  copy over files and run modules. Each time Ansible makes a new SSH connection to a
  host, it has to pay this negotiation penalty. OpenSSH is the most common implementation 
  of SSH; if you are on Linux or macOS, it is almost certainly the SSH client you have 
  installed on your local machine. OpenSSH supports an optimization called SSH multiplexing, 
  also referred to as ControlPersist, which allows multiple SSH sessions to the same host 
  to share the same TCP connection. This means that the TCP connection negotiation 
  happens only the first time, thus eliminating the negotiation penalty.

When you enable multiplexing, here is what happens:
• The first time you try to SSH to a host, OpenSSH starts one connection.
• OpenSSH creates a Unix domain socket (known as the control socket) that is
  associated with the remote host.
• The next time you try to SSH to a host, OpenSSH will use the control socket to
  communicate with the host instead of making a new TCP connection.

Ansible supports an optimization called pipelining. Pipelining, if supported by the
  connection plug-in, reduces the number of network operations required to execute
  a module on the remote server, by executing many Ansible modules without actual file 
  transfer. Ansible executes the Python scripts by piping them to the SSH session instead of 
  copying them. This saves time because it tells Ansible to use one SSH session instead of two.


For a long time, we had two types of management strategies for network devices:
• Buy an expensive proprietary software that configures your devices.
• Develop minimal tooling around your configuration files: back up your 
  configs locally, make some changes by editing them, and copy the result 
  back onto the devices through the console.

Staging
  Most organizations that develop software have a blueprint for staging. Staging means
  running separate environments for different purposes in the life cycle of software.
  You develop software on a virtual desktop, and the software is built on the dev
  environment, tested on the test environment, and then deployed for “acceptance” and
  eventually production. There are many ways to do this, but in general you’d like to
  find problems as early as possible. It is a good practice to use network separation and security 
  controls like firewalls, access management, and redundancy depicts such staging environments.

