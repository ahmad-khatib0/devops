 ▲                                                   ▲
 █ The ~> 4.0 syntax is equivalent to >= 4.0, < 5.0. █
 ▼                                                   ▼

There are four core values in the DevOps movement: culture, automation, measure‐
  ment, and sharing (sometimes abbreviated as the acronym CAMS). 

What Is Infrastructure as Code?:
The idea behind infrastructure as code (IaC) is that you write and execute code to
  define, deploy, update, and destroy your infrastructure. This represents an important
  shift in mindset in which you treat all aspects of operations as software—even those
  aspects that represent hardware (e.g., setting up physical servers). In fact, a key
  insight of DevOps is that you can manage almost everything in code, including
  servers, databases, networks, logfiles, application configuration, documentation, auto‐
  mated tests, deployment processes, and so on.

There are five broad categories of IaC tools:
• Ad hoc scripts
• Configuration management tools
• Server templating tools
• Orchestration tools
• Provisioning tools

The great thing about ad hoc scripts is that you can use popular, general-purpose
  programming languages, and you can write the code however you want. The terrible
  thing about ad hoc scripts is that you can use popular, general-purpose programming
  languages, and you can write the code however you want.


Configuration management tools: 
using a tool like Ansible offers a number of advantages:

Coding conventions
  Ansible enforces a consistent, predictable structure, including documentation, file layout, 
  clearly named parameters, secrets management, and so on. While every developer organizes 
  their ad hoc scripts in a different way, most configuration management tools come with a 
  set of conventions that makes it easier to navigate the code.
  
Idempotence
  Writing an ad hoc script that works once isn’t too difficult; writing an ad hoc script that works 
  correctly even if you run it over and over again is much harder. Every time you go to create a 
  folder in your script, you need to remember to check whether that folder already exists; every 
  time you add a line of configuration to a file, you need to check that line doesn’t already exist; 
  every time you want to run an app, you need to check that the app isn’t already running. Code that 
  works correctly no matter how many times you run it is called idempotent code. To make the Bash 
  script from the previous section idempotent, you’d need to add many lines of code, including lots 
  of if-statements. Most Ansible functions, on the other hand, are idempotent by default. For example, 
  the web-server.yml Ansible role will install Apache only if it isn’t installed already and
  will try to start the Apache web server only if it isn’t running already.
  
Distribution
  Ad hoc scripts are designed to run on a single, local machine. Ansible and other configuration 
  management tools are designed specifically for managing large numbers of remote servers



Server Templating Tools
  An alternative to configuration management that has been growing in popularity recently are 
  server templating tools such as Docker, Packer, and Vagrant. Instead of launching a bunch of 
  servers and configuring them by running the same code on each one, the idea behind server templating 
  tools is to create an image of a server that captures a fully self-contained “snapshot” of the 
  operating system (OS), the software, the files, and all other relevant details. You can then use 
  some other IaC tool to install that image on all of your servers


On most modern operating systems, code runs in one of two “spaces”: kernel space or user space. Code running 
  in kernel space has direct, unrestricted access to all of the hardware. There are no security restrictions
  (i.e., you can execute any CPU instruction, access any part of the hard drive, write to any address in memory)
  or safety restrictions (e.g., a crash in kernel space will typically crash the entire computer), so kernel space
  is generally reserved for the lowest-level, most trusted functions of the OS (typically called the kernel). Code
  running in user space does not have any direct access to the hardware and must use APIs exposed by the OS
  kernel instead. These APIs can enforce security restrictions (e.g., user permissions) and safety (e.g., a crash in
  a user space app typically affects only that app), so just about all application code runs in user space

Note that the different server templating tools have slightly different purposes. Packer is typically 
  used to create images that you run directly on top of production servers, such as an AMI that you 
  run in your production AWS account. Vagrant is typically used to create images that you run on 
  your development computers, such as a VirtualBox image that you run on your Mac or Windows laptop. 
  Docker is typically used to create images of individual applications. You can run the Docker images on
  production or development computers, as long as some other tool has configured that computer with the 
  Docker Engine. For example, a common pattern is to use Packer to create an AMI that has the Docker 
  Engine installed, deploy that AMI on a cluster of servers in your AWS account, and then deploy 
  individual Docker containers across that cluster to run your applications.

The idea behind immutable infrastructure is similar: once you’ve deployed a server, you never make 
  changes to it again. If you need to update something, such as deploying a new version of your code, 
  you create a new image from your server template and you deploy it on a new server. Because 
  servers never change, it’s a lot easier to reason about what’s deployed.
  
According to the 2016 State of DevOps Report, organizations that use DevOps practices, such as 
  IaC, deploy 200 times more frequently, recover from failures 24 times 
  faster, and have lead times that are 2,555 times lower.


Highlights two major problems with procedural IaC tools:
Procedural code does not fully capture the state of the infrastructure
  Reading through the three preceding Ansible templates is not enough to know what’s deployed. You’d 
  also need to know the order in which those templates were applied. Had you applied them in a different 
  order, you might have ended up with different infrastructure, and that’s not something you can see 
  in the codebase itself. In other words, to reason about an Ansible or Chef codebase, you need 
  to know the full history of every change that has ever happened.
Procedural code limits reusability
  The reusability of procedural code is inherently limited because you must man‐
  ually take into account the current state of the infrastructure. Because that state
  is constantly changing, code you used a week ago might no longer be usable
  because it was designed to modify a state of your infrastructure that no longer
  exists. As a result, procedural codebases tend to grow large and complicated over time.


General-Purpose Language Versus Domain-Specific Language
  Chef and Pulumi allow you to use a general-purpose programming language (GPL) to manage infrastructure 
  as code: Chef supports Ruby; Pulumi supports a wide variety of GPLs, including JavaScript, TypeScript, 
  Python, Go, C#, Java, and others. Terraform, Puppet, Ansible, CloudFormation, and OpenStack Heat each 
  use a domain-specific language (DSL) to manage infrastructure as code: Terraform uses HCL; Puppet uses 
  Puppet Language; Ansible, CloudFormation, and OpenStack Heat use YAML (CloudFormation also supports JSON).



Master Versus Masterless
  By default, Chef and Puppet require that you run a master server for storing the state of your 
  infrastructure and distributing updates. Every time you want to update something in your 
  infrastructure, you use a client (e.g., a command-line tool) to issue new commands to the master 
  server, and the master server either pushes the updates out to all of the other servers or those 
  servers pull the latest updates down from the master server on a regular basis.
  
Having to run a master server has some serious drawbacks:
Extra infrastructure
  You need to deploy an extra server, or even a cluster of extra servers (for high
  availability and scalability), just to run the master.
  
Maintenance
  You need to maintain, upgrade, back up, monitor, and scale the master server(s).
  
Security
  You need to provide a way for the client to communicate to the master server(s) and a way for the 
  master server(s) to communicate with all the other servers, which typically means opening extra ports 
  and configuring extra authentication systems, all of which increases your surface area to attacker


Ansible, CloudFormation, Heat, Terraform, and Pulumi are all masterless by default. Or, to be 
  more accurate, some of them rely on a master server, but it’s already part of the infrastructure 
  you’re using and not an extra piece that you need to manage. For example, Terraform communicates 
  with cloud providers using the cloud provider’s APIs, so in some sense, the API servers are master 
  servers, except that they don’t require any extra infrastructure or any extra authentication 
  mechanisms (i.e., just use your API keys). Ansible works by connecting directly to each server over 
  SSH, so again, you don’t need to run any extra infrastructure or manage extra authentication
  mechanisms (i.e., just use your SSH keys).



Agent Versus Agentless:
  Chef and Puppet require you to install agent software (e.g., Chef Client, Puppet Agent) on each 
  server that you want to configure. The agent typically runs in the background on each server and 
  is responsible for installing the latest configuration management updates. This has a few drawbacks:
Bootstrapping
  How do you provision your servers and install the agent software on them in the first place? 
  Some configuration management tools kick the can down the road, assuming that some external process 
  will take care of this for them (e.g., you first use Terraform to deploy a bunch of servers with 
  an AMI that has the agent already installed); other configuration management tools have a special
  bootstrapping process in which you run one-off commands to provision the servers using the cloud 
  provider APIs and install the agent software on those servers over SSH.
Maintenance
  You need to update the agent software on a periodic basis, being careful to keep it 
  synchronized with the master server if there is one. You also need to monitor the
  agent software and restart it if it crashes.
Security
  If the agent software pulls down configuration from a master server (or some other server if 
  you’re not using a master), you need to open outbound ports on every server. If the master server 
  pushes configuration to the agent, you need to open inbound ports on every server. In either case, 
  you must figure out how to authenticate the agent to the server to which it’s communicating. 
  All of this increases your surface area to attackers.


A Note on Default Virtual Private Clouds
  All of the AWS examples in this book use the Default VPC in your AWS account. A VPC, or virtual 
  private cloud, is an isolated area of your AWS account that has its own virtual network and IP
  address space. Just about every AWS resource deploys into a VPC. If you don’t explicitly specify 
  a VPC, the resource will be deployed into the Default VPC, which is part of every AWS account created
  after 2013. If for some reason you deleted the Default VPC in your account, either use a different 
  region (each region has its own Default VPC) or create a new Default VPC using the AWS Web
  Console. Otherwise, you’ll need to update almost every example
  to include a vpc_id or subnet_id parameter pointing to a custom VPC.


run terraform init to tell Terraform to scan the code, figure out which providers
  you’re using, and download the code for them. By default, the provider code will
  be downloaded into a .terraform folder, which is Terraform’s scratch directory (you
  may want to add it to .gitignore). Terraform will also record information about the
  provider code it downloaded into a .terraform.lock.hcl file

The -/+ in the plan output means “replace”; look for the text “forces replacement”
  in the plan output to figure out what is forcing Terraform to do a replacement.

The body of the variable declaration can contain the following optional parameters:
description
  It’s always a good idea to use this parameter to document how a variable is used.
  Your teammates will be able to see this description not only while reading the
  code but also when running the plan or apply commands 
  
default
  There are a number of ways to provide a value for the variable, including passing it in at the 
  command line (using the -var option), via a file (using the -var- file option), or via an 
  environment variable (Terraform looks for environment variables of the name TF_VAR_<variable_name>). 
  If no value is passed in, the variable will fall back to this default value. If there is no 
  default value, Terraform will interactively prompt the user for one.
  
type
  This allows you to enforce type constraints on the variables a user passes in. Terraform supports 
  a number of type constraints, including string, number, bool, list, map, set, object, tuple, and any. 
  It’s always a good idea to define a type constraint to catch simple errors. If you don’t 
  specify a type, Terraform assumes the type is any.
  
validation
  This allows you to define custom validation rules for the input variable that go
  beyond basic type checks, such as enforcing minimum or maximum values on a number .
  
sensitive
  If you set this parameter to true on an input variable, Terraform will not log it
  when you run plan or apply. You should use this on any secrets you pass into
  your Terraform code via variables: e.g., passwords, API keys, etc. 


In addition to input variables, Terraform also allows you to define output variables
  The CONFIG can contain the following optional parameters:

description
  It’s always a good idea to use this parameter to document what type of data is
  contained in the output variable.
sensitive
  Set this parameter to true to instruct Terraform not to log this output at the end of plan or 
  apply. This is useful if the output variable contains secrets such as passwords or private keys. 
  Note that if your output variable references an input variable or resource attribute marked with 
  sensitive = true, you are required to mark the output variable with sensitive = true as well to 
  indicate you are intentionally outputting a secret.
depends_on
  Normally, Terraform automatically figures out your dependency graph based
  on the references within your code, but in rare situations, you have to give it
  extra hints. For example, perhaps you have an output variable that returns the
  IP address of a server, but that IP won’t be accessible until a security group
  (firewall) is properly configured for that server. In that case, you may explicitly
  tell Terraform there is a dependency between the IP address output variable and
  the security group resource using depends_on.


AWS offers three types of load balancers:
Application Load Balancer (ALB)
  Best suited for load balancing of HTTP and HTTPS traffic. Operates at the
  application layer (Layer 7) of the Open Systems Interconnection (OSI) model.
  
Network Load Balancer (NLB)
  Best suited for load balancing of TCP, UDP, and TLS traffic. Can scale up and down in 
  response to load faster than the ALB (the NLB is designed to scale to tens of millions 
  of requests per second). Operates at the transport layer (Layer 4) of the OSI model.
  
Classic Load Balancer (CLB)
  This is the “legacy” load balancer that predates both the ALB and NLB. It can handle 
  HTTP, HTTPS, TCP, and TLS traffic but with far fewer features than either the ALB or NLB. 
  Operates at both the application layer (L7) and transport layer (L4) of the OSI model.


The ALB consists of several parts: 
Listener
  Listens on a specific port (e.g., 80) and protocol (e.g., HTTP)
  
Listener rule
  Takes requests that come into a listener and sends those that match specific paths (e.g., /foo and /bar) 
  or hostnames (e.g., foo.example.com and bar.exam ple.com) to specific target groups.

Target groups
  One or more servers that receive requests from the load balancer. The target
  group also performs health checks on these servers and sends requests only to healthy nodes.


There are two ways you could isolate state files:
Isolation via workspaces: 
  Useful for quick, isolated tests on the same configuration
  
Isolation via file layout:
  Useful for production use cases for which you need strong separation between environments


Workspaces drawbacks:
• The state files for all of your workspaces are stored in the same backend (e.g., the same S3 bucket). 
  That means you use the same authentication and access controls for all the workspaces, which is one 
  major reason workspaces are an unsuitable mechanism for isolating environments 
  (e.g., isolating staging from production).
  
• Workspaces are not visible in the code or on the terminal unless you run terra form workspace commands. 
  When browsing the code, a module that has been deployed in one workspace looks exactly the same as a 
  module deployed in 10 workspaces. This makes maintenance more difficult, because you don’t have a
  good picture of your infrastructure.
  
• Putting the two previous items together, the result is that workspaces can be fairly error prone. The lack 
  of visibility makes it easy to forget what workspace you’re in and accidentally deploy changes in the wrong 
  one (e.g., accidentally running terraform destroy in a “production” workspace rather than a “staging”
  workspace), and because you must use the same authentication mechanism for
  all workspaces, you have no other layers of defense to protect against such errors.


Isolation via File Layout
To achieve full isolation between environments, you need to do the following:
• Put the Terraform configuration files for each environment into a separate folder. For example, all 
  of the configurations for the staging environment can be in a folder called stage and all the 
  configurations for the production environment can be in a folder called prod.
  
• Configure a different backend for each environment, using different authentication mechanisms and access 
  controls: e.g., each environment could live in a separate AWS account with a separate S3 bucket as a backend.


terraform typical folder structure: 
stage
  An environment for pre-production workloads (i.e., testing)
  
prod
  An environment for production workloads (i.e., user-facing apps)
  
mgmt
  An environment for DevOps tooling (e.g., bastion host, CI server)
  
global
  A place to put resources that are used across all environments (e.g., S3, IAM) 
  Within each environment, there are separate folders for each “component.” The components differ for 
  every project, but here are the typical ones:
    vpc
      The network topology for this environment.
    services
      The apps or microservices to run in this environment, such as a Ruby on Rails frontend or a Scala 
      backend. Each app could even live in its own folder to isolate it from all the other apps.
    data-storage
      The data stores to run in this environment, such as MySQL or Redis. Each data
      store could even reside in its own folder to isolate it from all other data stores.
      Within each component, there are the actual Terraform configuration files, which are
      organized according to the following naming conventions:
        variables.tf
          Input variables
        outputs.tf
          Output variables
        main.tf
          Resources and data sources



terraform typical files names: 
dependencies.tf
  It’s common to put all your data sources in a dependencies.tf file to make it easier
  to see what external things the code depends on.
  
providers.tf
  You may want to put your provider blocks into a providers.tf file so you can see, at a glance, 
  what providers the code talks to and what authentication you’ll have to provide.
  
main-xxx.tf
  If the main.tf file is getting really long because it contains a large number of resources, you could 
  break it down into smaller files that group the resources in some logical way: e.g., main-iam.tf 
  could contain all the IAM resources, main- s3.tf could contain all the S3 resources, and so on. 
  Using the main- prefix makes it easier to scan the list of files in a folder when they are 
  organized alphabetically, as all the resources will be grouped together. It’s also worth noting that 
  if you find yourself managing a very large number of resources and struggling to break them down across 
  many files, that might be a sign that you should break your code into smaller modules instead,


Cleaning Up After Tests: 
  a common pattern is to run cloud-nuke as a cron job once per day in each sandbox environment to delete all 
  resources that are more than 48 hours old, based on the assumption that any infrastructure a developer fired 
  up for manual testing is no longer necessary after a couple of days:
      $ cloud-nuke aws --older-than 48h

The basic strategy for writing unit tests for Terraform is as follows:
  1. Create a small, standalone module.
  2. Create an easy-to-deploy example for that module.
  3. Run terraform apply to deploy the example into a real environment.
  4. Validate that what you just deployed works as expected. This step is specific to
     the type of infrastructure you’re testing: for example, for an ALB, you’d validate
     it by sending an HTTP request and checking that you receive back the expected response.
  5. Run terraform destroy at the end of the test to clean up.



For example, you can use tools such as tfsec and tflint to enforce policies, such as:
• Security groups cannot be too open: e.g., block inbound rules that allow access
  from all IPs (CIDR block 0.0.0.0/0).
• All EC2 Instances must follow a specific tagging convention.


Suppose that you have five copies of the old version of your app running, and you want to roll out a new version. 
 Here are a few of the most common strategies you can use::

Rolling deployment with replacement
  Take down one of the old copies of the app, deploy a new copy to replace it, wait for the new copy to come up 
  and pass health checks, start sending the new copy live traffic, and then repeat the process until all of the 
  old copies have been replaced. Rolling deployment with replacement ensures that you never have more than five 
  copies of the app running, which can be useful if you have limited capacity (e.g., if each copy of the app runs 
  on a physical server) or if you’re dealing with a stateful system where each app has a unique identity (e.g., this is
  often the case with consensus systems, such as Apache ZooKeeper). Note that this deployment strategy can work with 
  larger batch sizes (you can replace more than one copy of the app at a time if you can handle the load and won’t
  lose data with fewer apps running) and that during deployment, you will have both the old and new versions 
  of the app running at the same time.
  
Rolling deployment without replacement
  Deploy one new copy of the app, wait for the new copy to come up and pass health checks, start sending the new copy 
  live traffic, undeploy an old copy of the app, and then repeat the process until all the old copies have been replaced.
  Rolling deployment without replacement works only if you have flexible capacity (e.g., your apps run in the cloud, 
  where you can spin up new virtual servers any time you want) and if your application can tolerate more than five copies 
  of it running at the same time. The advantage is that you never have less than five copies of the app running, so you’re 
  not running at a reduced capacity during deployment. Note that this deployment strategy can also work with larger batch
  sizes (if you have the capacity for it, you can deploy five new copies all at once) and that during deployment, 
  you will have both the old and new versions of the app running at the same time.
  
Blue-green deployment
  Deploy five new copies of the app, wait for all of them to come up and pass health checks, shift all live traffic 
  to the new copies at the same time, and then undeploy the old copies. Blue-green deployment works only if you have 
  flexible capacity (e.g., your apps run in the cloud, where you can spin up new virtual servers any time you want) 
  and if your application can tolerate more than five copies of it running at the same time. The advantage is that 
  only one version of your app is visible to users at any given time and that you never have less than five copies 
  of the app running, so you’re not running at a reduced capacity during deployment.
 
Canary deployment
  Deploy one new copy of the app, wait for it to come up and pass health checks, start sending live traffic to it, 
  and then pause the deployment. During the pause, compare the new copy of the app, called the “canary,” to one of the 
  old copies, called the “control.” You can compare the canary and control across a variety of dimensions: CPU usage, 
  memory usage, latency, throughput, error rates in the logs, HTTP response codes, and so on. Ideally, there’s no way 
  to tell the two servers apart, which should give you confidence that the new code works just fine. In that case, 
  you unpause the deployment and use one of the rolling deploy‐ ment strategies to complete it. On the other hand, 
  if you spot any differences, then that may be a sign of problems in the new code, and you can cancel the deployment 
  and undeploy the canary before the problem becomes worse. The name comes from the “canary in a coal mine” concept, 
  where miners would take canary birds with them down into the tunnels, and if the tunnels filled with dangerous gases 
  (e.g., carbon monoxide), those gases would affect the canary before the miners, thus providing an early warning to 
  the miners that something was wrong and that they needed to exit immediately, before more damage was done. 
  The canary deployment offers similar benefits, giving you a systematic way to test new code in production in a way 
  that, if something goes wrong, you get a warning early on, when it has affected only a small portion of your users 
  and you still have enough time to react and prevent further damage. Canary deployments are often combined with feature 
  toggles, in which you wrap all new features in an if-statement. By default, the if-statement defaults to false, 
  so the new feature is toggled off when you initially deploy the code. Because all new functionality is off, when you 
  deploy the canary server, it should behave identically to the control, and any differences can be automatically flagged 
  as a problem and trigger a rollback. If there were no problems, later on you can enable the feature toggle for a portion 
  of your users via an internal web interface. For example, you might initially enable the new feature only for employees; 
  if that works well, you can enable it for 1% of users; if that’s still working well, you can ramp it up to 10%; 
  and so on. If at any point there’s a problem, you can use the feature toggle to ramp the feature back down. 
  This process allows you to separate deployment of new code from release of new features.



Promotion across environments: 
to roll out v0.0.4 of your app, you would do the following:
1. Deploy v0.0.4 of the app to dev.
2. Run your manual and automated tests in dev.
3. If v0.0.4 works well in dev, repeat steps 1 and 2 to deploy v0.0.4 to staging (this is known as promoting the artifact).
4. If v0.0.4 works well in staging, repeat steps 1 and 2 again to promote v0.0.4 to prod.

Here’s what the infrastructure code workflow looks like:
1. Use version control
2. Run the code locally
3. Make code changes
4. Submit changes for review
5. Run automated tests
6. Merge and release
7. Deploy

Version control for infrastructure code has a few extra requirements:
• Live repo and modules repo
• Golden Rule of Terraform
• The trouble with branches

Live repo and modules repo
  you will typically want at least two separate version control repositories for your Terraform code: one repo for 
  modules and one repo for live infrastructure. The repository for modules is where you create your reusable, versioned 
  modules,  such as (cluster/asg-rolling-deploy, data-stores/mysql, networking/alb, and services/hello-world-app). 
  The repository for live infrastructure defines the live infrastructure you’ve deployed in each environment 
  (dev, stage, prod, etc.)


The Golden Rule of Terraform
  Here’s a quick way to check the health of your Terraform code: go into your live repository, pick several folders 
  at random, and run terraform plan in each one. If the output is always “no changes,” that’s great, because it means 
  that your infrastructure code matches what’s actually deployed.
  The gold standard, or what you’re really aiming for, is what I call The Golden Rule of Terraform:
   “ The main branch of the live repository should be a 1:1 representation of what’s actually deployed in production “

“...what’s actually deployed”
  The only way to ensure that the Terraform code in the live repository is an up-to-date representation of what’s
  actually deployed is to never make out-of-band changes. After you begin using Terraform, do not make changes via 
  a web UI, or manual API calls, or any other mechanism. out-of-band changes not only lead to complicated bugs, but 
  they also void many of the benefits you get from using IaC in the first place.

“...a 1:1 representation...”
  If I browse your live repository, I should be able to see, from a quick scan, what resources have been deployed in 
  what environments. That is, every resource should have a 1:1 match with some line of code checked into the live repo. 
  This seems obvious at first glance, but it’s surprisingly easy to get it wrong. One way to get it wrong, as I just 
  mentioned, is to make out-of-band changes so that the code is there, but the live infrastructure is different. 
  A more subtle way to get it wrong is to use Terraform workspaces to manage environments so that the live infrastructure 
  is there, but the code isn’t. That is, if you use workspaces, your live repo will have only one copy of the code, even 
  though you may have 3 or 30 environments deployed with it. From merely looking at the code, there will be no way to know 
  what’s actually deployed, which will lead to mistakes and make maintenance complicated. Therefore, as described in 
  “Isolation via Workspaces” instead of using workspaces to manage environments, you want each environment defined in a 
  separate folder, using separate files, so that you can see exactly what environments have been deployed just by 
  browsing the live repository. Later in this chapter, you’ll see how to do this with minimal copying and pasting.

“The main branch...”
  You should have to look at only a single branch to understand what’s actually deployed in production. Typically, 
  that branch will be main. This means that all changes that affect the production environment should go directly 
  into main (you can create a separate branch but only to create a pull request with the intention of merging that 
  branch into main), and you should run terraform apply only for the production environment against the main branch. 


Atlantis is an open source tool that automatically runs terraform plan on 
    commits and adds the plan output to pull requests as a comment, 

Terraform, does not roll back automatically in case of errors. In part, that’s because there is no reasonable way 
  to roll back many types of infrastructure changes: for example, if an app deployment failed, it’s almost always 
  safe to roll back to an older version of the app, but if the Terraform change you were deploying failed, and that 
  change was to delete a database or terminate a server, you can’t easily roll that back!

Retries
  Certain types of Terraform errors are transient and go away if you rerun terra form apply. The deployment tooling you 
  use with Terraform should detect these known errors and automatically retry after a brief pause. Terragrunt has automatic 
  retries on known errors as a built-in feature. 
  
Terraform state errors
  Occasionally, Terraform will fail to save state after running terraform apply. For example, if you lose internet 
  connectivity partway through an apply, not only will the apply fail, but Terraform won’t be able to write the 
  updated state file to your remote backend (e.g., to Amazon S3). In these cases, Terraform will save the state file 
  on disk in a file called errored.tfstate. Make sure that your CI server does not delete these files 
  (e.g., as part of cleaning up the workspace after a build)! If you can still access this file after a failed deployment, 
  as soon as internet connectivity is restored, you can push this file to your remote backend (e.g., to S3) using the 
  state push command so that the state information isn’t lost:
$ terraform state push errored.tfstate

Errors releasing locks
  Occasionally, Terraform will fail to release a lock. For example, if your CI server crashes in the middle of a 
  terraform apply, the state will remain permanently locked. Anyone else who tries to run apply on the same module will 
  get an error message saying the state is locked and showing the ID of the lock. If you’re absolutely sure this is an 
  accidentally leftover lock, you can forcibly release it using the force-unlock command, passing it the ID of the lock 
  from that error message:
$ terraform force-unlock <LOCK_ID>

Deployment server: 
Lock the CI server down
  Make it accessible solely over HTTPs, require all users to be authenticated, and follow server-hardening practices 
  (e.g., lock down the firewall, install fail2ban, enable audit logging, etc.).

Don’t expose your CI server on the public internet
  That is, run the CI server in private subnets, without any public IP, so that it’s accessible only over a VPN connection. 
  That way, only users with valid network access (e.g., via a VPN certificate) can access your CI server at all. Note 
  that this does have a drawback: webhooks from external systems won’t work. For example, GitHub won’t automatically be 
  able to trigger builds in your CI server; instead, you’ll need to configure your CI server to poll your version 
  control system for updates. This is a small price to pay for a significantly more secure CI server.

Enforce an approval workflow
  Configure your CI/CD pipeline to require that every deployment be approved by at least one person (other than the 
  person who requested the deployment in the first place). During this approval step, the reviewer should be able to 
  see both the code changes and the plan output, as one final check that things look OK before apply runs. This ensures 
  that every deployment, code change, and plan output has had at least two sets of eyes on it.

Don’t give the CI server permanent credentials
  Instead of manually managed, permanent credentials (e.g., AWS access keys copy/pasted into your CI server), you 
  should prefer to use authentication mechanisms that use temporary credentials, such as IAM roles and OIDC.

Don’t give the CI server admin credentials
  Instead, isolate the admin credentials to a totally separate, isolated worker: e.g., a separate server, a separate 
  container, etc. That worker should be extremely locked down, so no developers have access to it at all, and the only 
  thing it allows is for the CI server to trigger that worker via an extremely limited remote API. For example, that 
  worker’s API may only allow you to run specific commands (e.g., terraform plan and terraform apply), in specific repos 
  (e.g., your live repo), in specific branches (e.g., the main branch), and so on. This way, even if an attacker gets 
  access to your CI server, they still won’t have access to the admin credentials, and all they can do is request a 
  deployment on some code that’s already in your version control system, which isn’t nearly as much of a catastrophe
  as leaking the admin credentials fully

Promote artifacts across environments
  Just as with application artifacts, you’ll want to promote your immutable, versioned infrastructure artifacts from 
  environment to environment: for example, promote v0.0.6 from dev to stage to prod.7 The rule here is also simple:
  Always test Terraform changes in pre-prod before prod.
  The process for promoting Terraform code across environments is similar to the process of promoting application 
  artifacts, except there is an extra approval step, where you run terraform plan and have someone manually 
  review the output and approve the deployment
  Here’s what the process looks like for promoting, for instance, v0.0.6 of a Terraform
  module across the dev, stage, and prod environments:

1. Update the dev environment to v0.0.6, and run terraform plan.
2. Prompt someone to review and approve the plan; for example, send an automated message via Slack.
3. If the plan is approved, deploy v0.0.6 to dev by running terraform apply.
4. Run your manual and automated tests in dev.
5. If v0.0.6 works well in dev, repeat steps 1–4 to promote v0.0.6 to staging.
6. If v0.0.6 works well in staging, repeat steps 1–4 again to promote v0.0.6 to production.


Terragrunt allows you to define all of your Terraform code exactly once in the modules repo, whereas in the live repo, 
  you will have solely terragrunt.hcl files that provide a DRY way to configure and deploy each module in each 
  environment. This will result in a live repo with far fewer files and lines of code





